{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "880a3b72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x7fe44401d100>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Written by Christopher Straub\n",
    "# inspired from Francois Fleuret <francois@fleuret.org> code (practical3)\n",
    "\n",
    "import math\n",
    "from torch import empty\n",
    "from torch import set_grad_enabled\n",
    "import numpy as np\n",
    "import dlc_practical_prologue as prologue\n",
    "\n",
    "set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85659c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Module(object):\n",
    "    def forward(self, *input):\n",
    "        raise NotImplementedError\n",
    "    def backward(self, *gradwrtoutput): \n",
    "        raise NotImplementedError\n",
    "    def param(self): \n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ee468d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(Module):\n",
    "    def __init__(self,input_size, hidden_size):\n",
    "        super().__init__()\n",
    "        epsilon = 1e-6\n",
    "        self.hidden_size = hidden_size\n",
    "        self.input_size = input_size\n",
    "        self.weights=(empty(input_size, hidden_size).normal_(0,epsilon))#Weight\n",
    "        self.biais=( empty(hidden_size).normal_(0,epsilon)) #bias\n",
    "\n",
    "    def sigma(self,x):\n",
    "        out = 0\n",
    "        if len(x.size())>1:\n",
    "            \n",
    "            out = x.mm(self.weights)+self.biais.view(1,-1)*(empty(x.size(0),self.hidden_size).zero_().add(1))\n",
    "        else : \n",
    "            out = self.weights.t().mv(x)+self.biais\n",
    "        return out \n",
    "    def dsigma(self,x):\n",
    "        out = 0\n",
    "        if len(x.size())>1:\n",
    "            out = x.mm(self.weights.T)\n",
    "        else : \n",
    "            out = self.weights().mv(x)\n",
    "            \n",
    "        return out\n",
    "    def param(self):\n",
    "        return [self.weights,self.biais]\n",
    "    def set_param(self,new_w,new_b):\n",
    "        self.weights= new_w\n",
    "        self.biais = new_b\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e45fb139",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tanh(Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    def sigma(self,x):\n",
    "        return x.tanh()\n",
    "    def dsigma(self,x):\n",
    "        return 4 * (x.exp() + x.mul(-1).exp()).pow(-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b6fef23",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSE(Module):  \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    def sigma(self,x,y):\n",
    "\n",
    "        \n",
    "        return (x - y).pow(2).sum()\n",
    "    def dsigma(self,x,y):\n",
    "        #if len(y.size())>1:\n",
    "        #    y = y.argmax(0)\n",
    "            \n",
    "        return 2*(x - y)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0bd67cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss(Module):  \n",
    "    def __init__(self,loss,net):\n",
    "        super().__init__()\n",
    "        self.net = net\n",
    "        self.loss = loss\n",
    "        self.acc_loss=0\n",
    "        self.nb_train_errors=0\n",
    "    def sigma(self,x,y):\n",
    "        return self.loss.sigma(x,y)\n",
    "    def dsigma(self,x,y):\n",
    "        return self.dloss.sigma(x,y)\n",
    "    def assign(self, net):\n",
    "        self.net =net \n",
    "    #def prediction(self, *input):\n",
    "    #    return self.loss.sigma(self.net.forward_value)\n",
    "    \n",
    "    def backward(self,*gradwrtoutput): \n",
    "        x,s = self.net.forward(self.net.train)\n",
    "    \n",
    "        #print(self.net.train_target.size())\n",
    "        for n in range(self.net.num_sample):\n",
    "            x2 = x[-1][n]\n",
    "            pred = x2.max(0)[1].item()\n",
    "            #print('n',pred)\n",
    "            if self.net.train_target[n][ pred] < 0.5:\n",
    "                self.nb_train_errors +=  1\n",
    "        \n",
    "            self.acc_loss+=  self.loss.sigma(x2, self.net.train_target[n])\n",
    "        \n",
    "        gradwrtoutput = [self.net.train_target,[x,s],self.loss]\n",
    "        self.net.backward(*gradwrtoutput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "51e38a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossEntropy(Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    def sigma(self,x,y):\n",
    "        y_=y.argmax().item()\n",
    "        return -(x[y_].exp().div(x.exp().sum()).log())\n",
    "    def dsigma(self,x,y):\n",
    "        y_=y.argmax().item()\n",
    "        out= x.exp().div(x.exp().sum())\n",
    "        out[y_]=1-x[y_].exp().div(x.exp().sum())\n",
    "        return out \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb4cc1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Relu( Module ) :\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    def sigma(self,x):\n",
    "        return x.max(empty(x.size()).zero_())\n",
    "    def dsigma(self,x):\n",
    "        out = empty(x.size()).zero_()\n",
    "        out[x>0]=1\n",
    "        return out\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e3927d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.Parameters = []\n",
    "        self.Activation = []\n",
    "        self.dl_dw = []\n",
    "        self.dl_db = []\n",
    "        self.train = []\n",
    "        self.train_target = []\n",
    "        self.forward_value=[]\n",
    "        self.num_sample = 0\n",
    "    def forward(self,*input):\n",
    "        train_input=input[0]\n",
    "        x = train_input\n",
    "        out_s = []\n",
    "        out_x = [train_input]\n",
    "        for i in range(len(self.Activation)):\n",
    "            s = self.Parameters[i].sigma(x)\n",
    "            out_s.append(s)\n",
    "            x = self.Activation[i].sigma(s)\n",
    "            out_x.append(x)\n",
    "        return [out_x,out_s]\n",
    "\n",
    "    def backward(self,*gradwrtoutput):\n",
    "        train_target,layer_output,Loss = gradwrtoutput\n",
    "        dl_dw = self.dl_dw\n",
    "        dl_db = self.dl_db\n",
    "        N=len(dl_dw)\n",
    "        x,s=layer_output\n",
    "        x0 = x[0]\n",
    "        dl_dx2 = Loss.dsigma(x[N], train_target)\n",
    "        #print('dl_dx_2',dl_dx2.size())\n",
    "        #print('s[N-1]',s[N-1].size())\n",
    "        #print(self.Activation[N-1])\n",
    "        dl_ds2 = self.Activation[N-1].dsigma(s[N-1]) * dl_dx2\n",
    "        dl_dw2 = dl_dw[N-1]\n",
    "        dl_db2 = dl_db[N-1]\n",
    "        #print('dl_ds2',dl_ds2.size())\n",
    "        #print('x[N-1]',x[N-1].size())\n",
    "        \n",
    "        #print('dl_dw2',dl_dw2.size())\n",
    "        dl_dw2.add_( x[N-1].view(x[N-1].size(0),x[N-1].size(1),1).matmul(dl_ds2.view(dl_ds2.size(0),1,dl_ds2.size(1))))\n",
    "        dl_db2.add_(dl_ds2)\n",
    "        out_dl_dw = [dl_dw2]\n",
    "        out_dl_db = [dl_db2]\n",
    "\n",
    "        for i in range(1,N,1):\n",
    "\n",
    "            dl_dx1 = self.Parameters[N-i].dsigma(dl_ds2) #c'est pas très jolie mais par Parameters je veux dire la fonction devrivant les poids de la couche cache (ici linear)\n",
    "            dl_ds1 = self.Activation[N-1-i].dsigma(s[N-1-i]) * dl_dx1\n",
    "            dl_dw1 = dl_dw[N-1-i]\n",
    "            dl_db1 = dl_db[N-1-i]\n",
    "             \n",
    "            dl_dw1.add_(x[N-1-i].view(x[N-1-i].size(0),x[N-1-i].size(1),1).matmul(dl_ds1.view(dl_ds1.size(0),1,dl_ds1.size(1))))\n",
    "            dl_db1.add_(dl_ds1)\n",
    "            out_dl_dw.insert(0,dl_dw1)\n",
    "            out_dl_db.insert(0,dl_db1)\n",
    "            dl_ds2 = dl_ds1\n",
    "        self.dl_dw,self.dl_db = out_dl_dw,out_dl_db\n",
    "    def param(self):\n",
    "        return self.Parameters\n",
    "    def init(self,new_Parameters,new_Activation):\n",
    "        self.Parameters= new_Parameters\n",
    "        self.Activation = new_Activation\n",
    "\n",
    "    def set_param(self,num_layer,new_w,new_b):\n",
    "        self.Parameters[num_layer].set_param(new_w,new_b)\n",
    "    def get_grad(self,num_layer):\n",
    "        return [self.dl_dw[num_layer],self.dl_db[num_layer]]\n",
    "    def zero_grad(self):\n",
    "        for dw in self.dl_dw:\n",
    "            dw.zero_()\n",
    "        for db in self.dl_db:\n",
    "            db.zero_()\n",
    "\n",
    "    def assign(self, train,train_target):\n",
    "        \n",
    "        if len(train_input.size())>1 : \n",
    "            self.train = train\n",
    "            self.train_target = train_target\n",
    "            self.num_sample = train.size(0)\n",
    "            self.dl_dw = [empty(self.num_sample,p.param()[0].size(0),p.param()[0].size(1)) for p in self.Parameters]\n",
    "            self.dl_db = [empty(self.num_sample,p.param()[1].size(0)) for p in self.Parameters]\n",
    "        else : \n",
    "            self.num_sample = 1\n",
    "            self.train = train\n",
    "            self.train_target = train_target\n",
    "            self.dl_dw = [empty(p.param()[0].size()) for p in self.Parameters]\n",
    "            self.dl_db = [empty(p.param()[1].size()) for p in self.Parameters]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2bc7ef14",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequential(Module):\n",
    "    def __init__(self,*layers):\n",
    "        super().__init__()\n",
    "        self.layers = layers \n",
    "    def init_net(self):\n",
    "        net = Net()\n",
    "        new_Parameters=[]\n",
    "        new_Activation=[]\n",
    "        i = 0\n",
    "        for layer in self.layers : \n",
    "            if np.mod(i,2)==0:\n",
    "                new_Parameters.append(layer)\n",
    "            else : \n",
    "                new_Activation.append(layer)\n",
    "            i=i+1\n",
    "        net.init(new_Parameters,new_Activation)\n",
    "        return net "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e67fc30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(a):\n",
    "    num_class = a.max()+1\n",
    "    N=a.size(0)\n",
    "    out = empty(N,num_class).zero_()\n",
    "    for i in range(N) :\n",
    "        out[i][a[i]]=1 \n",
    "    return out \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "621b3df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_disc_set(nb):\n",
    "    input = empty(nb, 2).uniform_(-1, 1)\n",
    "    target = input.pow(2).sum(1).sub(2 / math.pi).sign().add(1).div(2).long()\n",
    "    \n",
    "    return input, one_hot(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e76ae257",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 acc_train_loss 1000.00 acc_train_error 49.00% test_error 52.10%\n",
      "1 acc_train_loss 989.70 acc_train_error 49.00% test_error 52.10%\n",
      "2 acc_train_loss 979.81 acc_train_error 49.00% test_error 52.10%\n",
      "3 acc_train_loss 970.31 acc_train_error 49.00% test_error 52.10%\n",
      "4 acc_train_loss 961.18 acc_train_error 49.00% test_error 52.10%\n",
      "5 acc_train_loss 952.42 acc_train_error 49.00% test_error 52.10%\n",
      "6 acc_train_loss 944.02 acc_train_error 49.00% test_error 52.10%\n",
      "7 acc_train_loss 935.92 acc_train_error 49.00% test_error 52.10%\n",
      "8 acc_train_loss 928.16 acc_train_error 49.00% test_error 52.10%\n",
      "9 acc_train_loss 920.70 acc_train_error 49.00% test_error 52.10%\n",
      "10 acc_train_loss 913.55 acc_train_error 49.00% test_error 52.10%\n",
      "11 acc_train_loss 906.66 acc_train_error 49.00% test_error 52.10%\n",
      "12 acc_train_loss 900.07 acc_train_error 49.00% test_error 52.10%\n",
      "13 acc_train_loss 893.72 acc_train_error 49.00% test_error 52.10%\n",
      "14 acc_train_loss 887.64 acc_train_error 49.00% test_error 52.10%\n",
      "15 acc_train_loss 881.78 acc_train_error 49.00% test_error 52.10%\n",
      "16 acc_train_loss 876.16 acc_train_error 49.00% test_error 52.10%\n",
      "17 acc_train_loss 870.76 acc_train_error 49.00% test_error 52.10%\n",
      "18 acc_train_loss 865.58 acc_train_error 49.00% test_error 52.10%\n",
      "19 acc_train_loss 860.61 acc_train_error 49.00% test_error 52.10%\n",
      "20 acc_train_loss 855.83 acc_train_error 49.00% test_error 52.10%\n",
      "21 acc_train_loss 851.23 acc_train_error 49.00% test_error 52.10%\n",
      "22 acc_train_loss 846.83 acc_train_error 49.00% test_error 52.10%\n",
      "23 acc_train_loss 842.60 acc_train_error 49.00% test_error 52.10%\n",
      "24 acc_train_loss 838.52 acc_train_error 49.00% test_error 52.10%\n",
      "25 acc_train_loss 834.62 acc_train_error 49.00% test_error 52.10%\n",
      "26 acc_train_loss 830.86 acc_train_error 49.00% test_error 52.10%\n",
      "27 acc_train_loss 827.26 acc_train_error 49.00% test_error 52.10%\n",
      "28 acc_train_loss 823.81 acc_train_error 49.00% test_error 52.10%\n",
      "29 acc_train_loss 820.49 acc_train_error 49.00% test_error 52.10%\n",
      "30 acc_train_loss 817.29 acc_train_error 49.00% test_error 52.10%\n",
      "31 acc_train_loss 814.22 acc_train_error 49.00% test_error 52.10%\n",
      "32 acc_train_loss 811.28 acc_train_error 49.00% test_error 52.10%\n",
      "33 acc_train_loss 808.45 acc_train_error 49.00% test_error 52.10%\n",
      "34 acc_train_loss 805.75 acc_train_error 49.00% test_error 52.10%\n",
      "35 acc_train_loss 803.14 acc_train_error 49.00% test_error 52.10%\n",
      "36 acc_train_loss 800.63 acc_train_error 49.00% test_error 52.10%\n",
      "37 acc_train_loss 798.22 acc_train_error 49.00% test_error 52.10%\n",
      "38 acc_train_loss 795.92 acc_train_error 49.00% test_error 52.10%\n",
      "39 acc_train_loss 793.70 acc_train_error 49.00% test_error 52.10%\n",
      "40 acc_train_loss 791.56 acc_train_error 49.00% test_error 52.10%\n",
      "41 acc_train_loss 789.53 acc_train_error 49.00% test_error 52.10%\n",
      "42 acc_train_loss 787.56 acc_train_error 49.00% test_error 52.10%\n",
      "43 acc_train_loss 785.67 acc_train_error 49.00% test_error 52.10%\n",
      "44 acc_train_loss 783.86 acc_train_error 49.00% test_error 52.10%\n",
      "45 acc_train_loss 782.12 acc_train_error 49.00% test_error 52.10%\n",
      "46 acc_train_loss 780.44 acc_train_error 49.00% test_error 52.10%\n",
      "47 acc_train_loss 778.84 acc_train_error 49.00% test_error 52.10%\n",
      "48 acc_train_loss 777.30 acc_train_error 49.00% test_error 52.10%\n",
      "49 acc_train_loss 775.82 acc_train_error 49.00% test_error 52.10%\n",
      "50 acc_train_loss 774.40 acc_train_error 49.00% test_error 52.10%\n",
      "51 acc_train_loss 773.03 acc_train_error 49.00% test_error 52.10%\n",
      "52 acc_train_loss 771.72 acc_train_error 49.00% test_error 52.10%\n",
      "53 acc_train_loss 770.46 acc_train_error 49.00% test_error 52.10%\n",
      "54 acc_train_loss 769.24 acc_train_error 49.00% test_error 52.10%\n",
      "55 acc_train_loss 768.08 acc_train_error 49.00% test_error 52.10%\n",
      "56 acc_train_loss 766.97 acc_train_error 49.00% test_error 52.10%\n",
      "57 acc_train_loss 765.90 acc_train_error 49.00% test_error 52.10%\n",
      "58 acc_train_loss 764.87 acc_train_error 49.00% test_error 52.10%\n",
      "59 acc_train_loss 763.88 acc_train_error 49.00% test_error 52.10%\n",
      "60 acc_train_loss 762.93 acc_train_error 49.00% test_error 52.10%\n",
      "61 acc_train_loss 762.02 acc_train_error 49.00% test_error 52.10%\n",
      "62 acc_train_loss 761.15 acc_train_error 49.00% test_error 52.10%\n",
      "63 acc_train_loss 760.30 acc_train_error 49.00% test_error 52.10%\n",
      "64 acc_train_loss 759.49 acc_train_error 49.00% test_error 52.10%\n",
      "65 acc_train_loss 758.72 acc_train_error 49.00% test_error 52.10%\n",
      "66 acc_train_loss 757.97 acc_train_error 49.00% test_error 52.10%\n",
      "67 acc_train_loss 757.25 acc_train_error 49.00% test_error 52.10%\n",
      "68 acc_train_loss 756.57 acc_train_error 49.00% test_error 52.10%\n",
      "69 acc_train_loss 755.91 acc_train_error 49.00% test_error 52.10%\n",
      "70 acc_train_loss 755.27 acc_train_error 49.00% test_error 52.10%\n",
      "71 acc_train_loss 754.67 acc_train_error 49.00% test_error 52.10%\n",
      "72 acc_train_loss 754.08 acc_train_error 49.00% test_error 52.10%\n",
      "73 acc_train_loss 753.52 acc_train_error 49.00% test_error 52.10%\n",
      "74 acc_train_loss 752.98 acc_train_error 49.00% test_error 52.10%\n",
      "75 acc_train_loss 752.46 acc_train_error 49.00% test_error 52.10%\n",
      "76 acc_train_loss 751.97 acc_train_error 49.00% test_error 52.10%\n",
      "77 acc_train_loss 751.48 acc_train_error 49.00% test_error 52.10%\n",
      "78 acc_train_loss 751.03 acc_train_error 49.00% test_error 52.10%\n",
      "79 acc_train_loss 750.59 acc_train_error 49.00% test_error 52.10%\n",
      "80 acc_train_loss 750.17 acc_train_error 49.00% test_error 52.10%\n",
      "81 acc_train_loss 749.76 acc_train_error 49.00% test_error 52.10%\n",
      "82 acc_train_loss 749.37 acc_train_error 49.00% test_error 52.10%\n",
      "83 acc_train_loss 749.00 acc_train_error 49.00% test_error 52.10%\n",
      "84 acc_train_loss 748.63 acc_train_error 49.00% test_error 52.10%\n",
      "85 acc_train_loss 748.28 acc_train_error 49.00% test_error 52.10%\n",
      "86 acc_train_loss 747.96 acc_train_error 49.00% test_error 52.10%\n",
      "87 acc_train_loss 747.63 acc_train_error 49.00% test_error 52.10%\n",
      "88 acc_train_loss 747.33 acc_train_error 49.00% test_error 52.10%\n",
      "89 acc_train_loss 747.04 acc_train_error 49.00% test_error 52.10%\n",
      "90 acc_train_loss 746.75 acc_train_error 49.00% test_error 52.10%\n",
      "91 acc_train_loss 746.48 acc_train_error 49.00% test_error 52.10%\n",
      "92 acc_train_loss 746.22 acc_train_error 49.00% test_error 52.10%\n",
      "93 acc_train_loss 745.97 acc_train_error 49.00% test_error 52.10%\n",
      "94 acc_train_loss 745.72 acc_train_error 49.00% test_error 52.10%\n",
      "95 acc_train_loss 745.51 acc_train_error 49.00% test_error 52.10%\n",
      "96 acc_train_loss 745.27 acc_train_error 49.00% test_error 52.10%\n",
      "97 acc_train_loss 745.06 acc_train_error 49.00% test_error 52.10%\n",
      "98 acc_train_loss 744.87 acc_train_error 49.00% test_error 52.10%\n",
      "99 acc_train_loss 744.66 acc_train_error 49.00% test_error 52.10%\n",
      "100 acc_train_loss 744.47 acc_train_error 49.00% test_error 52.10%\n",
      "101 acc_train_loss 744.30 acc_train_error 49.00% test_error 52.10%\n",
      "102 acc_train_loss 744.12 acc_train_error 49.00% test_error 52.10%\n",
      "103 acc_train_loss 743.95 acc_train_error 49.00% test_error 52.10%\n",
      "104 acc_train_loss 743.79 acc_train_error 49.00% test_error 52.10%\n",
      "105 acc_train_loss 743.64 acc_train_error 49.00% test_error 52.10%\n",
      "106 acc_train_loss 743.49 acc_train_error 49.00% test_error 52.10%\n",
      "107 acc_train_loss 743.35 acc_train_error 49.00% test_error 52.10%\n",
      "108 acc_train_loss 743.21 acc_train_error 49.00% test_error 52.10%\n",
      "109 acc_train_loss 743.07 acc_train_error 49.00% test_error 52.10%\n",
      "110 acc_train_loss 742.95 acc_train_error 49.00% test_error 52.10%\n",
      "111 acc_train_loss 742.83 acc_train_error 49.00% test_error 52.10%\n",
      "112 acc_train_loss 742.72 acc_train_error 49.00% test_error 52.10%\n",
      "113 acc_train_loss 742.61 acc_train_error 49.00% test_error 52.10%\n",
      "114 acc_train_loss 742.50 acc_train_error 49.00% test_error 52.10%\n",
      "115 acc_train_loss 742.40 acc_train_error 49.00% test_error 52.10%\n",
      "116 acc_train_loss 742.30 acc_train_error 49.00% test_error 52.10%\n",
      "117 acc_train_loss 742.20 acc_train_error 49.00% test_error 52.10%\n",
      "118 acc_train_loss 742.11 acc_train_error 49.00% test_error 52.10%\n",
      "119 acc_train_loss 742.02 acc_train_error 49.00% test_error 52.10%\n",
      "120 acc_train_loss 741.93 acc_train_error 49.00% test_error 52.10%\n",
      "121 acc_train_loss 741.86 acc_train_error 49.00% test_error 52.10%\n",
      "122 acc_train_loss 741.79 acc_train_error 49.00% test_error 52.10%\n",
      "123 acc_train_loss 741.71 acc_train_error 49.00% test_error 52.10%\n",
      "124 acc_train_loss 741.64 acc_train_error 49.00% test_error 52.10%\n",
      "125 acc_train_loss 741.56 acc_train_error 49.00% test_error 52.10%\n",
      "126 acc_train_loss 741.50 acc_train_error 49.00% test_error 52.10%\n",
      "127 acc_train_loss 741.44 acc_train_error 49.00% test_error 52.10%\n",
      "128 acc_train_loss 741.38 acc_train_error 49.00% test_error 52.10%\n",
      "129 acc_train_loss 741.32 acc_train_error 49.00% test_error 52.10%\n",
      "130 acc_train_loss 741.25 acc_train_error 49.00% test_error 52.10%\n",
      "131 acc_train_loss 741.21 acc_train_error 49.00% test_error 52.10%\n",
      "132 acc_train_loss 741.16 acc_train_error 49.00% test_error 52.10%\n",
      "133 acc_train_loss 741.10 acc_train_error 49.00% test_error 52.10%\n",
      "134 acc_train_loss 741.06 acc_train_error 49.00% test_error 52.10%\n",
      "135 acc_train_loss 741.01 acc_train_error 49.00% test_error 52.10%\n",
      "136 acc_train_loss 740.97 acc_train_error 49.00% test_error 52.10%\n",
      "137 acc_train_loss 740.93 acc_train_error 49.00% test_error 52.10%\n",
      "138 acc_train_loss 740.89 acc_train_error 49.00% test_error 52.10%\n",
      "139 acc_train_loss 740.84 acc_train_error 49.00% test_error 52.10%\n",
      "140 acc_train_loss 740.81 acc_train_error 49.00% test_error 52.10%\n",
      "141 acc_train_loss 740.77 acc_train_error 49.00% test_error 52.10%\n",
      "142 acc_train_loss 740.73 acc_train_error 49.00% test_error 52.10%\n",
      "143 acc_train_loss 740.71 acc_train_error 49.00% test_error 52.10%\n",
      "144 acc_train_loss 740.67 acc_train_error 49.00% test_error 52.10%\n",
      "145 acc_train_loss 740.64 acc_train_error 49.00% test_error 52.10%\n",
      "146 acc_train_loss 740.62 acc_train_error 49.00% test_error 52.10%\n",
      "147 acc_train_loss 740.58 acc_train_error 49.00% test_error 52.10%\n",
      "148 acc_train_loss 740.56 acc_train_error 49.00% test_error 52.10%\n",
      "149 acc_train_loss 740.54 acc_train_error 49.00% test_error 52.10%\n",
      "150 acc_train_loss 740.51 acc_train_error 49.00% test_error 52.10%\n",
      "151 acc_train_loss 740.49 acc_train_error 49.00% test_error 52.10%\n",
      "152 acc_train_loss 740.46 acc_train_error 49.00% test_error 52.10%\n",
      "153 acc_train_loss 740.44 acc_train_error 49.00% test_error 52.10%\n",
      "154 acc_train_loss 740.41 acc_train_error 49.00% test_error 52.10%\n",
      "155 acc_train_loss 740.40 acc_train_error 49.00% test_error 52.10%\n",
      "156 acc_train_loss 740.38 acc_train_error 49.00% test_error 52.10%\n",
      "157 acc_train_loss 740.36 acc_train_error 49.00% test_error 52.10%\n",
      "158 acc_train_loss 740.33 acc_train_error 49.00% test_error 52.10%\n",
      "159 acc_train_loss 740.32 acc_train_error 49.00% test_error 52.10%\n",
      "160 acc_train_loss 740.31 acc_train_error 49.00% test_error 52.10%\n",
      "161 acc_train_loss 740.29 acc_train_error 49.00% test_error 52.10%\n",
      "162 acc_train_loss 740.28 acc_train_error 49.00% test_error 52.10%\n",
      "163 acc_train_loss 740.27 acc_train_error 49.00% test_error 52.10%\n",
      "164 acc_train_loss 740.24 acc_train_error 49.00% test_error 52.10%\n",
      "165 acc_train_loss 740.23 acc_train_error 49.00% test_error 52.10%\n",
      "166 acc_train_loss 740.22 acc_train_error 49.00% test_error 52.10%\n",
      "167 acc_train_loss 740.21 acc_train_error 49.00% test_error 52.10%\n",
      "168 acc_train_loss 740.19 acc_train_error 49.00% test_error 52.10%\n",
      "169 acc_train_loss 740.19 acc_train_error 49.00% test_error 52.10%\n",
      "170 acc_train_loss 740.17 acc_train_error 49.00% test_error 52.10%\n",
      "171 acc_train_loss 740.16 acc_train_error 49.00% test_error 52.10%\n",
      "172 acc_train_loss 740.15 acc_train_error 49.00% test_error 52.10%\n",
      "173 acc_train_loss 740.14 acc_train_error 49.00% test_error 52.10%\n",
      "174 acc_train_loss 740.13 acc_train_error 49.00% test_error 52.10%\n",
      "175 acc_train_loss 740.12 acc_train_error 49.00% test_error 52.10%\n",
      "176 acc_train_loss 740.11 acc_train_error 49.00% test_error 52.10%\n",
      "177 acc_train_loss 740.10 acc_train_error 49.00% test_error 52.10%\n",
      "178 acc_train_loss 740.10 acc_train_error 49.00% test_error 52.10%\n",
      "179 acc_train_loss 740.08 acc_train_error 49.00% test_error 52.10%\n",
      "180 acc_train_loss 740.08 acc_train_error 49.00% test_error 52.10%\n",
      "181 acc_train_loss 740.07 acc_train_error 49.00% test_error 52.10%\n",
      "182 acc_train_loss 740.07 acc_train_error 49.00% test_error 52.10%\n",
      "183 acc_train_loss 740.06 acc_train_error 49.00% test_error 52.10%\n",
      "184 acc_train_loss 740.06 acc_train_error 49.00% test_error 52.10%\n",
      "185 acc_train_loss 740.04 acc_train_error 49.00% test_error 52.10%\n",
      "186 acc_train_loss 740.04 acc_train_error 49.00% test_error 52.10%\n",
      "187 acc_train_loss 740.04 acc_train_error 49.00% test_error 52.10%\n",
      "188 acc_train_loss 740.03 acc_train_error 49.00% test_error 52.10%\n",
      "189 acc_train_loss 740.02 acc_train_error 49.00% test_error 52.10%\n",
      "190 acc_train_loss 740.02 acc_train_error 49.00% test_error 52.10%\n",
      "191 acc_train_loss 740.02 acc_train_error 49.00% test_error 52.10%\n",
      "192 acc_train_loss 740.01 acc_train_error 49.00% test_error 52.10%\n",
      "193 acc_train_loss 740.01 acc_train_error 49.00% test_error 52.10%\n",
      "194 acc_train_loss 740.00 acc_train_error 49.00% test_error 52.10%\n",
      "195 acc_train_loss 740.00 acc_train_error 49.00% test_error 52.10%\n",
      "196 acc_train_loss 739.99 acc_train_error 49.00% test_error 52.10%\n",
      "197 acc_train_loss 739.99 acc_train_error 49.00% test_error 52.10%\n",
      "198 acc_train_loss 739.99 acc_train_error 49.00% test_error 52.10%\n",
      "199 acc_train_loss 739.98 acc_train_error 49.00% test_error 52.10%\n",
      "200 acc_train_loss 739.98 acc_train_error 49.00% test_error 52.10%\n",
      "201 acc_train_loss 739.98 acc_train_error 49.00% test_error 52.10%\n",
      "202 acc_train_loss 739.97 acc_train_error 49.00% test_error 52.10%\n",
      "203 acc_train_loss 739.98 acc_train_error 49.00% test_error 52.10%\n",
      "204 acc_train_loss 739.97 acc_train_error 49.00% test_error 52.10%\n",
      "205 acc_train_loss 739.96 acc_train_error 49.00% test_error 52.10%\n",
      "206 acc_train_loss 739.96 acc_train_error 49.00% test_error 52.10%\n",
      "207 acc_train_loss 739.96 acc_train_error 49.00% test_error 52.10%\n",
      "208 acc_train_loss 739.96 acc_train_error 49.00% test_error 52.10%\n",
      "209 acc_train_loss 739.96 acc_train_error 49.00% test_error 52.10%\n",
      "210 acc_train_loss 739.95 acc_train_error 49.00% test_error 52.10%\n",
      "211 acc_train_loss 739.95 acc_train_error 49.00% test_error 52.10%\n",
      "212 acc_train_loss 739.95 acc_train_error 49.00% test_error 52.10%\n",
      "213 acc_train_loss 739.95 acc_train_error 49.00% test_error 52.10%\n",
      "214 acc_train_loss 739.94 acc_train_error 49.00% test_error 52.10%\n",
      "215 acc_train_loss 739.94 acc_train_error 49.00% test_error 52.10%\n",
      "216 acc_train_loss 739.94 acc_train_error 49.00% test_error 52.10%\n",
      "217 acc_train_loss 739.95 acc_train_error 49.00% test_error 52.10%\n",
      "218 acc_train_loss 739.94 acc_train_error 49.00% test_error 52.10%\n",
      "219 acc_train_loss 739.94 acc_train_error 49.00% test_error 52.10%\n",
      "220 acc_train_loss 739.94 acc_train_error 49.00% test_error 52.10%\n",
      "221 acc_train_loss 739.94 acc_train_error 49.00% test_error 52.10%\n",
      "222 acc_train_loss 739.94 acc_train_error 49.00% test_error 52.10%\n",
      "223 acc_train_loss 739.94 acc_train_error 49.00% test_error 52.10%\n",
      "224 acc_train_loss 739.93 acc_train_error 49.00% test_error 52.10%\n",
      "225 acc_train_loss 739.92 acc_train_error 49.00% test_error 52.10%\n",
      "226 acc_train_loss 739.92 acc_train_error 49.00% test_error 52.10%\n",
      "227 acc_train_loss 739.93 acc_train_error 49.00% test_error 52.10%\n",
      "228 acc_train_loss 739.93 acc_train_error 49.00% test_error 52.10%\n",
      "229 acc_train_loss 739.92 acc_train_error 49.00% test_error 52.10%\n",
      "230 acc_train_loss 739.93 acc_train_error 49.00% test_error 52.10%\n",
      "231 acc_train_loss 739.92 acc_train_error 49.00% test_error 52.10%\n",
      "232 acc_train_loss 739.92 acc_train_error 49.00% test_error 52.10%\n",
      "233 acc_train_loss 739.92 acc_train_error 49.00% test_error 52.10%\n",
      "234 acc_train_loss 739.92 acc_train_error 49.00% test_error 52.10%\n",
      "235 acc_train_loss 739.92 acc_train_error 49.00% test_error 52.10%\n",
      "236 acc_train_loss 739.92 acc_train_error 49.00% test_error 52.10%\n",
      "237 acc_train_loss 739.92 acc_train_error 49.00% test_error 52.10%\n",
      "238 acc_train_loss 739.92 acc_train_error 49.00% test_error 52.10%\n",
      "239 acc_train_loss 739.92 acc_train_error 49.00% test_error 52.10%\n",
      "240 acc_train_loss 739.92 acc_train_error 49.00% test_error 52.10%\n",
      "241 acc_train_loss 739.92 acc_train_error 49.00% test_error 52.10%\n",
      "242 acc_train_loss 739.92 acc_train_error 49.00% test_error 52.10%\n",
      "243 acc_train_loss 739.91 acc_train_error 49.00% test_error 52.10%\n",
      "244 acc_train_loss 739.91 acc_train_error 49.00% test_error 52.10%\n",
      "245 acc_train_loss 739.91 acc_train_error 49.00% test_error 52.10%\n",
      "246 acc_train_loss 739.91 acc_train_error 49.00% test_error 52.10%\n",
      "247 acc_train_loss 739.91 acc_train_error 49.00% test_error 52.10%\n",
      "248 acc_train_loss 739.92 acc_train_error 49.00% test_error 52.10%\n",
      "249 acc_train_loss 739.92 acc_train_error 49.00% test_error 52.10%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "train_input, train_target = generate_disc_set(1000)\n",
    "test_input, test_target = generate_disc_set(1000)\n",
    "\n",
    "mean, std = train_input.mean(), train_input.std()\n",
    "\n",
    "train_input.sub_(mean).div_(std)\n",
    "test_input.sub_(mean).div_(std)\n",
    "\n",
    "mini_batch_size = 100\n",
    "\n",
    "\n",
    "nb_classes = train_target.size(1)\n",
    "nb_train_samples = train_input.size(0)\n",
    "\n",
    "zeta = 0.90\n",
    "\n",
    "\n",
    "\n",
    "nb_hidden = 25\n",
    "eta = 1e-1 / nb_train_samples\n",
    "epsilon = 1e-6\n",
    "\n",
    "\n",
    "net = Sequential(Linear (train_input.size(1),25),Relu(),Linear( 25,25),Tanh(),Linear( 25,nb_classes),Relu()).init_net()\n",
    "\n",
    "loss = Loss(MSE(),net)\n",
    "\n",
    "\n",
    "nb_epochs = 250\n",
    "\n",
    "for e in range(nb_epochs):\n",
    "    for b in range(0, train_input.size(0), mini_batch_size):\n",
    "\n",
    "    # Back-prop\n",
    "\n",
    "        net.assign(train_input.narrow(0, b, mini_batch_size),train_target.narrow(0, b, mini_batch_size))\n",
    "        net.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "    # Gradient step\n",
    "   \n",
    "    for i in range(len(net.param())):\n",
    "        dl_dw ,dl_db = net.get_grad(i)\n",
    "        new_w=net.param()[i].param()[0]-eta * dl_dw.sum(0)  #en faisant que n(t,b) sot séquentielle\n",
    "        new_b=net.param()[i].param()[1]-eta* dl_db.sum(0) \n",
    "        net.set_param(i,new_w,new_b)\n",
    "\n",
    "    \n",
    "    # Test error\n",
    "\n",
    "    nb_test_errors = 0\n",
    "    \n",
    "    for n in range(test_input.size(0)):\n",
    "        x,s = net.forward(test_input[n])\n",
    "        x2 = x[-1]\n",
    "        pred = x2.max(0)[1].item()\n",
    "        if test_target[n][ pred] < 0.5:\n",
    "            nb_test_errors +=  1\n",
    "       \n",
    "\n",
    "    print('{:d} acc_train_loss {:.02f} acc_train_error {:.02f}% test_error {:.02f}%'\n",
    "          .format(e,\n",
    "                  loss.acc_loss,\n",
    "                  (100 * loss.nb_train_errors) / train_input.size(0),\n",
    "                  (100 * nb_test_errors) / test_input.size(0)))\n",
    "    loss.nb_train_errors  = 0\n",
    "    loss.acc_loss=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a56781c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MSE logging the loss signifie -> faire un entrainement avec log(mse) ou bien apres l'entrianemetn pour l'évaluation apprliquer le log ?? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9398e14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# que signifie 3 couche caché avec 25 unité ? \n",
    "# comment faire en sorte que le predict soit bien ? \n",
    "# vaut mieux avoir une sortie a savoir une valeur et voir si elle se rapproche de la valeur recherché ? \n",
    "# faire un plot avec avec un entrainement d'un ensemble de valeur dans un carré uniforme et voir quel sont les valeur correctment classifié \n",
    "# est ce que j'utilise vrm la puissance des tenseur ??? "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
