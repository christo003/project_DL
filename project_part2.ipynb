{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "880a3b72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x7fa8d4583730>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Written by Christopher Straub\n",
    "# inspired from Francois Fleuret <francois@fleuret.org> code (practical3)\n",
    "\n",
    "import math\n",
    "from torch import empty\n",
    "from torch import set_grad_enabled\n",
    "import numpy as np\n",
    "import dlc_practical_prologue as prologue\n",
    "\n",
    "set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85659c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Module(object):\n",
    "    def forward(self, *input):\n",
    "        raise NotImplementedError\n",
    "    def backward(self, *gradwrtoutput): \n",
    "        raise NotImplementedError\n",
    "    def param(self): \n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ee468d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(Module):\n",
    "    def __init__(self,input_size, hidden_size):\n",
    "        super().__init__()\n",
    "        epsilon = 1e-6\n",
    "          \n",
    "        self.weights=(empty( hidden_size,input_size).normal_(0,epsilon))#Weight\n",
    "        self.biais=( empty(hidden_size).normal_(0,epsilon)) #bias\n",
    "\n",
    "    def sigma(self,x):\n",
    "        return self.weights.mv(x)+self.biais\n",
    "    def dsigma(self,x):\n",
    "        return self.weights.t().mv(x)\n",
    "    def param(self):\n",
    "        return [self.weights,self.biais]\n",
    "    def set_param(self,new_w,new_b):\n",
    "        self.weights= new_w\n",
    "        self.biais = new_b\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e45fb139",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tanh(Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    def sigma(self,x):\n",
    "        return x.tanh()\n",
    "    def dsigma(self,x):\n",
    "        return 4 * (x.exp() + x.mul(-1).exp()).pow(-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b6fef23",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSE(Module):  \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = []\n",
    "    def sigma(self,x,y):\n",
    "        return (x - y).pow(2).sum()\n",
    "    def dsigma(self,x,y):\n",
    "        return 2*(x - y)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0bd67cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss(Module):  \n",
    "    def __init__(self,loss,net):\n",
    "        super().__init__()\n",
    "        self.net = net\n",
    "        self.loss = loss\n",
    "        self.acc_loss=0\n",
    "        self.nb_train_errors=0\n",
    "    def sigma(self,x,y):\n",
    "        return self.loss.sigma(x,y)\n",
    "    def dsigma(self,x,y):\n",
    "        return self.dloss.sigma(x,y)\n",
    "    def assign(self, net):\n",
    "        self.net =net \n",
    "    #def prediction(self, *input):\n",
    "    #    return self.loss.sigma(self.net.forward_value)\n",
    "    \n",
    "    def backward(self,*gradwrtoutput): \n",
    "        x,s = self.net.forward(self.net.train)\n",
    "        x2 = x[-1]\n",
    "        #pred = x2.max(0)[1].item()\n",
    "        if (self.net.train_target-x2).abs() < 0.5:\n",
    "            self.nb_train_errors +=  1\n",
    "        \n",
    "        self.acc_loss += self.loss.sigma(x2, self.net.train_target)\n",
    "        \n",
    "        gradwrtoutput = [self.net.train_target,[x,s],self.loss]\n",
    "        self.net.backward(*gradwrtoutput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "51e38a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossEntropy(Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    def sigma(self,x,y):\n",
    "        y_=y.argmax().item()\n",
    "        return -(x[y_].exp().div(x.exp().sum()).log())\n",
    "    def dsigma(self,x,y):\n",
    "        y_=y.argmax().item()\n",
    "        out= x.exp().div(x.exp().sum())\n",
    "        out[y_]=1-x[y_].exp().div(x.exp().sum())\n",
    "        return out \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "865680d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Relu( Module ) :\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    def sigma(self,x):\n",
    "        return x.max(empty(x.size(0)).zero_())\n",
    "    def dsigma(self,x):\n",
    "        out = x\n",
    "        out[x>0]=1\n",
    "        return out\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e3927d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.Parameters = []\n",
    "        self.Activation = []\n",
    "        self.dl_dw = []\n",
    "        self.dl_db = []\n",
    "        self.train = []\n",
    "        self.train_target = []\n",
    "        self.forward_value=[]\n",
    "        \n",
    "    def forward(self,*input):\n",
    "        train_input=input[0]\n",
    "        x = train_input\n",
    "        out_s = []\n",
    "        out_x = [train_input]\n",
    "        for i in range(len(self.Activation)):\n",
    "            s = self.Parameters[i].sigma(x)\n",
    "            out_s.append(s)\n",
    "            x = self.Activation[i].sigma(s)\n",
    "            out_x.append(x)\n",
    "        return [out_x,out_s]\n",
    "\n",
    "    def backward(self,*gradwrtoutput):\n",
    "        train_target,layer_output,Loss = gradwrtoutput\n",
    "        dl_dw = self.dl_dw\n",
    "        dl_db = self.dl_db\n",
    "        N=len(dl_dw)\n",
    "        x,s=layer_output\n",
    "        x0 = x[0]\n",
    "        dl_dx2 = Loss.dsigma(x[N], train_target)\n",
    "        dl_ds2 = self.Activation[N-1].dsigma(s[N-1]) * dl_dx2\n",
    "        dl_dw2 = dl_dw[N-1]\n",
    "        dl_db2 = dl_db[N-1]\n",
    "        dl_dw2.add_(dl_ds2.view(-1, 1).mm(x[N-1].view(1, -1)))\n",
    "        dl_db2.add_(dl_ds2)\n",
    "        out_dl_dw = [dl_dw2]\n",
    "        out_dl_db = [dl_db2]\n",
    "\n",
    "        for i in range(1,N,1):\n",
    "\n",
    "            dl_dx1 = self.Parameters[N-i].dsigma(dl_ds2)\n",
    "            dl_ds1 = self.Activation[N-1-i].dsigma(s[N-1-i]) * dl_dx1\n",
    "            dl_dw1 = dl_dw[N-1-i]\n",
    "            dl_db1 = dl_db[N-1-i]\n",
    "            dl_dw1.add_(dl_ds1.view(-1, 1).mm(x[N-1-i].view(1, -1)))\n",
    "            dl_db1.add_(dl_ds1)\n",
    "            out_dl_dw.insert(0,dl_dw1)\n",
    "            out_dl_db.insert(0,dl_db1)\n",
    "            dl_ds2 = dl_ds1\n",
    "        self.dl_dw,self.dl_db = out_dl_dw,out_dl_db\n",
    "    def param(self):\n",
    "        return self.Parameters\n",
    "    def init(self,new_Parameters,new_Activation):\n",
    "        self.Parameters= new_Parameters\n",
    "        self.Activation = new_Activation\n",
    "        self.dl_dw = [empty(p.param()[0].size()) for p in new_Parameters]\n",
    "        self.dl_db = [empty(p.param()[1].size()) for p in new_Parameters]\n",
    "    def set_param(self,num_layer,new_w,new_b):\n",
    "        self.Parameters[num_layer].set_param(new_w,new_b)\n",
    "    def get_grad(self,num_layer):\n",
    "        return [self.dl_dw[num_layer],self.dl_db[num_layer]]\n",
    "    def zero_grad(self):\n",
    "        for dw in self.dl_dw:\n",
    "            dw.zero_()\n",
    "        for db in self.dl_db:\n",
    "            db.zero_()\n",
    "\n",
    "    def assign(self, train,train_target):\n",
    "        self.train = train\n",
    "        self.train_target = train_target\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2bc7ef14",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequential(Module):\n",
    "    def __init__(self,*layers):\n",
    "        super().__init__()\n",
    "        self.layers = layers \n",
    "    def init_net(self):\n",
    "        net = Net()\n",
    "        new_Parameters=[]\n",
    "        new_Activation=[]\n",
    "        i = 0\n",
    "        for layer in self.layers : \n",
    "            if np.mod(i,2)==0:\n",
    "                new_Parameters.append(layer)\n",
    "            else : \n",
    "                new_Activation.append(layer)\n",
    "            i=i+1\n",
    "        net.init(new_Parameters,new_Activation)\n",
    "        return net "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ed92f7f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(a):\n",
    "    num_class = a.max()+1\n",
    "    N=a.size(0)\n",
    "    out = empty(N,num_class).zero_()\n",
    "    for i in range(N) :\n",
    "        out[i][a[i]]=1 \n",
    "    return out \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f108239e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_disc_set(nb):\n",
    "    input = empty(nb, 2).uniform_(-1, 1)\n",
    "    target = input.pow(2).sum(1).sub(2 / math.pi).sign().add(1).div(2).long()\n",
    "    \n",
    "    return input, target#one_hot(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e76ae257",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 acc_train_loss 510.00 acc_train_error 49.00% test_error 52.10%\n",
      "1 acc_train_loss 416.65 acc_train_error 49.00% test_error 52.10%\n",
      "2 acc_train_loss 358.26 acc_train_error 49.00% test_error 52.10%\n",
      "3 acc_train_loss 321.92 acc_train_error 49.00% test_error 52.10%\n",
      "4 acc_train_loss 298.91 acc_train_error 49.00% test_error 52.10%\n",
      "5 acc_train_loss 283.99 acc_train_error 49.00% test_error 52.10%\n",
      "6 acc_train_loss 274.06 acc_train_error 49.00% test_error 52.10%\n",
      "7 acc_train_loss 267.30 acc_train_error 49.00% test_error 52.10%\n",
      "8 acc_train_loss 262.60 acc_train_error 49.00% test_error 52.10%\n",
      "9 acc_train_loss 259.28 acc_train_error 49.00% test_error 52.10%\n",
      "10 acc_train_loss 256.89 acc_train_error 49.00% test_error 52.10%\n",
      "11 acc_train_loss 255.15 acc_train_error 49.00% test_error 52.10%\n",
      "12 acc_train_loss 253.88 acc_train_error 49.00% test_error 52.10%\n",
      "13 acc_train_loss 252.92 acc_train_error 49.00% test_error 52.10%\n",
      "14 acc_train_loss 252.21 acc_train_error 49.00% test_error 52.10%\n",
      "15 acc_train_loss 251.68 acc_train_error 49.00% test_error 52.10%\n",
      "16 acc_train_loss 251.27 acc_train_error 49.00% test_error 52.10%\n",
      "17 acc_train_loss 250.96 acc_train_error 49.00% test_error 52.10%\n",
      "18 acc_train_loss 250.73 acc_train_error 49.00% test_error 52.10%\n",
      "19 acc_train_loss 250.54 acc_train_error 49.00% test_error 52.10%\n",
      "20 acc_train_loss 250.40 acc_train_error 49.00% test_error 52.10%\n",
      "21 acc_train_loss 250.29 acc_train_error 49.00% test_error 52.10%\n",
      "22 acc_train_loss 250.20 acc_train_error 49.00% test_error 52.10%\n",
      "23 acc_train_loss 250.14 acc_train_error 49.00% test_error 52.10%\n",
      "24 acc_train_loss 250.09 acc_train_error 49.00% test_error 52.10%\n",
      "25 acc_train_loss 250.05 acc_train_error 49.00% test_error 52.10%\n",
      "26 acc_train_loss 250.02 acc_train_error 49.00% test_error 47.90%\n",
      "27 acc_train_loss 249.99 acc_train_error 51.00% test_error 47.90%\n",
      "28 acc_train_loss 249.97 acc_train_error 51.00% test_error 47.90%\n",
      "29 acc_train_loss 249.96 acc_train_error 51.00% test_error 47.90%\n",
      "30 acc_train_loss 249.95 acc_train_error 51.00% test_error 47.90%\n",
      "31 acc_train_loss 249.94 acc_train_error 51.00% test_error 47.90%\n",
      "32 acc_train_loss 249.93 acc_train_error 51.00% test_error 47.90%\n",
      "33 acc_train_loss 249.92 acc_train_error 51.00% test_error 47.90%\n",
      "34 acc_train_loss 249.92 acc_train_error 51.00% test_error 47.90%\n",
      "35 acc_train_loss 249.91 acc_train_error 51.00% test_error 47.90%\n",
      "36 acc_train_loss 249.91 acc_train_error 51.00% test_error 47.90%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1155/4246221454.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_input\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_target\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;31m#gradwrtoutput = [train_target[n],[x,s],loss]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;31m# Gradient step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_1155/433617037.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, *gradwrtoutput)\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnb_train_errors\u001b[0m \u001b[0;34m+=\u001b[0m  \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macc_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigma\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_target\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mgradwrtoutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_target\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_1155/1442994179.py\u001b[0m in \u001b[0;36msigma\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msigma\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdsigma\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "train_input, train_target = generate_disc_set(1000)\n",
    "test_input, test_target = generate_disc_set(1000)\n",
    "\n",
    "mean, std = train_input.mean(), train_input.std()\n",
    "\n",
    "train_input.sub_(mean).div_(std)\n",
    "test_input.sub_(mean).div_(std)\n",
    "\n",
    "mini_batch_size = 100\n",
    "\n",
    "\n",
    "nb_classes = 1#train_target.size(1)\n",
    "nb_train_samples = train_input.size(0)\n",
    "\n",
    "zeta = 0.90\n",
    "\n",
    "\n",
    "\n",
    "nb_hidden = 25\n",
    "eta = 1e-1 / nb_train_samples\n",
    "epsilon = 1e-6\n",
    "\n",
    "\n",
    "net = Sequential(Linear (train_input.size(1),25),Tanh(),Linear( 25,25),Relu(),Linear( 25,nb_classes),Tanh()).init_net()\n",
    "\n",
    "loss = Loss(MSE(),net)\n",
    "\n",
    "\n",
    "for k in range(1000):\n",
    "\n",
    "    # Back-prop\n",
    "\n",
    "\n",
    "    \n",
    "    net.zero_grad()\n",
    "\n",
    "\n",
    "    for n in range(nb_train_samples):  \n",
    "        \n",
    "        net.assign(train_input[n],train_target[n])\n",
    "        #gradwrtoutput = [train_target[n],[x,s],loss]\n",
    "        loss.backward()\n",
    "\n",
    "    # Gradient step\n",
    "   \n",
    "    for i in range(len(net.param())):\n",
    "        dl_dw ,dl_db = net.get_grad(i)\n",
    "        new_w=net.param()[i].param()[0]-eta * dl_dw \n",
    "        new_b=net.param()[i].param()[1]-eta* dl_db \n",
    "        net.set_param(i,new_w,new_b)\n",
    "\n",
    "    \n",
    "    # Test error\n",
    "\n",
    "    nb_test_errors = 0\n",
    "    \n",
    "    for n in range(test_input.size(0)):\n",
    "        input = test_input[n]\n",
    "        x,s = net.forward(input)\n",
    "        x2 = x[-1]\n",
    "        #pred = x2.max(0)[1].item()\n",
    "        if (test_target[n]-x2).abs() < 0.5:\n",
    "            nb_test_errors = nb_test_errors + 1\n",
    "\n",
    "    print('{:d} acc_train_loss {:.02f} acc_train_error {:.02f}% test_error {:.02f}%'\n",
    "          .format(k,\n",
    "                  loss.acc_loss,\n",
    "                  (100 * loss.nb_train_errors) / train_input.size(0),\n",
    "                  (100 * nb_test_errors) / test_input.size(0)))\n",
    "    loss.nb_train_errors  = 0\n",
    "    loss.acc_loss=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5f07e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MSE logging the loss signifie -> faire un entrainement avec log(mse) ou bien apres l'entrianemetn pour l'évaluation apprliquer le log ?? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9ad033",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a faire le SGD ! \n",
    "# pourquoi mon erreur augmente ??? \n",
    "# que signifie 3 couche caché avec 25 unité ? \n",
    "# comment faire en sorte que le predict soit bien ? \n",
    "# vaut mieux avoir une sortie a savoir une valeur et voir si elle se rapproche de la valeur recherché ? \n",
    "# faire un plot avec avec un entrainement d'un ensemble de valeur dans un carré uniforme et voir quel sont les valeur correctment classifié \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
