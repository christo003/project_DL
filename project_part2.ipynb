{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "880a3b72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x7f854c6055e0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Written by Christopher Straub\n",
    "# inspired from Francois Fleuret <francois@fleuret.org> code (practical3)\n",
    "\n",
    "import math\n",
    "import torch\n",
    "import numpy as np\n",
    "import dlc_practical_prologue as prologue\n",
    "\n",
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85659c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Module(object):\n",
    "    def forward(self, *input):\n",
    "        raise NotImplementedError\n",
    "    def backward(self, *gradwrtoutput): \n",
    "        raise NotImplementedError\n",
    "    def param(self): \n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ee468d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(Module):\n",
    "    def __init__(self,input_size, hidden_size):\n",
    "        super().__init__()\n",
    "        epsilon = 1e-6\n",
    "          \n",
    "        self.weights=(torch.empty( hidden_size,input_size).normal_(0,epsilon))#Weight\n",
    "        self.biais=( torch.empty(hidden_size).normal_(0,epsilon)) #bias\n",
    "        #self.input_size = input_size\n",
    "        #self.hidden_size = hidden_size\n",
    "    def sigma(self,x):\n",
    "        return self.weights.mv(x)+self.biais\n",
    "    def dsigma(self,x):\n",
    "        return self.weights.t().mv(x)\n",
    "    def param(self):\n",
    "        return [self.weights,self.biais]\n",
    "    def set_param(self,new_w,new_b):\n",
    "        self.weights= new_w\n",
    "        self.biais = new_b\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e45fb139",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tanh(Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    def sigma(self,x):\n",
    "        return x.tanh()\n",
    "    def dsigma(self,x):\n",
    "        return 4 * (x.exp() + x.mul(-1).exp()).pow(-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b6fef23",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossMSE(Module): #le loss doit il appartenir au module ? \n",
    "    def __init__(self):#,*net):\n",
    "        super().__init__()\n",
    "        #self.net=net\n",
    "    def sigma(self,x,y):\n",
    "        return (x - y).pow(2).sum() \n",
    "    def dsigma(self,x,y):\n",
    "        return 2*(x - y)\n",
    "    #def backward(self,*gradwrtoutput):\n",
    "    #    net=self.net[0]\n",
    "    #    net.gradwrtoutput[-1].add_(self.dloss(net.forward(*net.train),*net.target))\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e3927d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.Parameters = []\n",
    "        self.Activation = []\n",
    "    def forward(self,*input):#(w1, b1, w2, b2, x):\n",
    "        train_input=input[0]\n",
    "        x = train_input\n",
    "        out_s = []\n",
    "        out_x = [train_input]\n",
    "        for i in range(len(self.Activation)):\n",
    "            s = self.Parameters[i].sigma(x)#Parameters[i].param()[0].mv(x)+Parameters[i].param()[1]\n",
    "            out_s.append(s)\n",
    "            x = self.Activation[i].sigma(s)\n",
    "            out_x.append(x)\n",
    "        return [out_x,out_s]\n",
    "\n",
    "    def backward(self,*gradwrtoutput):#(w1, b1, w2, b2,t,x, s1, x1, s2, x2,dl_dw1, dl_db1, dl_dw2, dl_db2):\n",
    "        train_target,layer_output,dl_dw,dl_db,Loss = gradwrtoutput\n",
    "        N=len(dl_dw)\n",
    "        x,s=layer_output\n",
    "        x0 = x[0]\n",
    "        dl_dx2 = Loss.dsigma(x[N], train_target)\n",
    "        dl_ds2 = self.Activation[N-1].dsigma(s[N-1]) * dl_dx2\n",
    "        dl_dw2 = dl_dw[N-1]\n",
    "        dl_db2 = dl_db[N-1]\n",
    "        dl_dw2.add_(dl_ds2.view(-1, 1).mm(x[N-1].view(1, -1)))\n",
    "        dl_db2.add_(dl_ds2)\n",
    "        out_dl_dw = [dl_dw2]\n",
    "        out_dl_db = [dl_db2]\n",
    "\n",
    "        for i in range(1,N,1):\n",
    "\n",
    "            dl_dx1 = self.Parameters[N-i].dsigma(dl_ds2)#Parameters[N-i].param()[0].t().mv(dl_ds2)\n",
    "            dl_ds1 = self.Activation[N-1-i].dsigma(s[N-1-i]) * dl_dx1\n",
    "            dl_dw1 = dl_dw[N-1-i]\n",
    "            dl_db1 = dl_db[N-1-i]\n",
    "            dl_dw1.add_(dl_ds1.view(-1, 1).mm(x[N-1-i].view(1, -1)))\n",
    "            dl_db1.add_(dl_ds1)\n",
    "            out_dl_dw.insert(0,dl_dw1)\n",
    "            out_dl_db.insert(0,dl_db1)\n",
    "            dl_ds2 = dl_ds1\n",
    "        return out_dl_dw,out_dl_db\n",
    "    def param(self):\n",
    "        return self.Parameters\n",
    "    def init(self,new_Parameters,new_Activation):\n",
    "        self.Parameters= new_Parameters\n",
    "        self.Activation = new_Activation\n",
    "    def set_param(self,num_layer,new_w,new_b):\n",
    "        self.Parameters[num_layer].set_param(new_w,new_b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9dbb3bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequential(Module):\n",
    "    def __init__(self,*layers):\n",
    "        super().__init__()\n",
    "        self.layers = layers \n",
    "    def init_net(self):\n",
    "        net = Net()\n",
    "        new_Parameters=[]\n",
    "        new_Activation=[]\n",
    "        i = 0\n",
    "        for layer in self.layers : \n",
    "            if np.mod(i,2)==0:\n",
    "                new_Parameters.append(layer)\n",
    "            else : \n",
    "                new_Activation.append(layer)\n",
    "            i=i+1\n",
    "        net.init(new_Parameters,new_Activation)\n",
    "        return net "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e76ae257",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Using MNIST\n",
      "** Reduce the data-set (use --full for the full thing)\n",
      "** Use 1000 train and 1000 test samples\n",
      "0 acc_train_loss 810.00 acc_train_error 90.30% test_error 90.10%\n",
      "1 acc_train_loss 780.58 acc_train_error 88.30% test_error 90.10%\n",
      "2 acc_train_loss 761.77 acc_train_error 88.30% test_error 90.10%\n",
      "3 acc_train_loss 749.74 acc_train_error 88.30% test_error 90.10%\n",
      "4 acc_train_loss 742.04 acc_train_error 88.30% test_error 90.10%\n",
      "5 acc_train_loss 737.11 acc_train_error 88.30% test_error 90.10%\n",
      "6 acc_train_loss 733.95 acc_train_error 88.30% test_error 90.10%\n",
      "7 acc_train_loss 731.92 acc_train_error 88.30% test_error 90.10%\n",
      "8 acc_train_loss 730.62 acc_train_error 88.30% test_error 90.10%\n",
      "9 acc_train_loss 729.78 acc_train_error 88.30% test_error 90.10%\n",
      "10 acc_train_loss 729.25 acc_train_error 88.30% test_error 90.10%\n",
      "11 acc_train_loss 728.90 acc_train_error 88.30% test_error 90.10%\n",
      "12 acc_train_loss 728.68 acc_train_error 88.30% test_error 90.10%\n",
      "13 acc_train_loss 728.53 acc_train_error 88.30% test_error 90.10%\n",
      "14 acc_train_loss 728.44 acc_train_error 88.30% test_error 90.10%\n",
      "15 acc_train_loss 728.38 acc_train_error 88.30% test_error 90.10%\n",
      "16 acc_train_loss 728.34 acc_train_error 88.30% test_error 90.10%\n",
      "17 acc_train_loss 728.31 acc_train_error 88.30% test_error 90.10%\n",
      "18 acc_train_loss 728.30 acc_train_error 88.30% test_error 90.10%\n",
      "19 acc_train_loss 728.28 acc_train_error 88.30% test_error 90.10%\n",
      "20 acc_train_loss 728.26 acc_train_error 88.30% test_error 90.10%\n",
      "21 acc_train_loss 728.24 acc_train_error 88.30% test_error 90.10%\n",
      "22 acc_train_loss 728.22 acc_train_error 88.30% test_error 90.10%\n",
      "23 acc_train_loss 728.16 acc_train_error 88.30% test_error 90.10%\n",
      "24 acc_train_loss 728.08 acc_train_error 88.30% test_error 90.10%\n",
      "25 acc_train_loss 727.92 acc_train_error 88.30% test_error 90.10%\n",
      "26 acc_train_loss 727.65 acc_train_error 88.30% test_error 82.70%\n",
      "27 acc_train_loss 727.16 acc_train_error 79.90% test_error 79.20%\n",
      "28 acc_train_loss 726.31 acc_train_error 78.40% test_error 79.00%\n",
      "29 acc_train_loss 724.83 acc_train_error 78.70% test_error 76.90%\n",
      "30 acc_train_loss 722.31 acc_train_error 72.60% test_error 74.50%\n",
      "31 acc_train_loss 718.15 acc_train_error 71.00% test_error 74.00%\n",
      "32 acc_train_loss 711.64 acc_train_error 71.00% test_error 74.00%\n",
      "33 acc_train_loss 702.24 acc_train_error 71.70% test_error 74.40%\n",
      "34 acc_train_loss 690.00 acc_train_error 71.90% test_error 74.30%\n",
      "35 acc_train_loss 675.95 acc_train_error 71.60% test_error 74.10%\n",
      "36 acc_train_loss 661.58 acc_train_error 70.70% test_error 72.70%\n",
      "37 acc_train_loss 647.80 acc_train_error 69.80% test_error 71.70%\n",
      "38 acc_train_loss 634.84 acc_train_error 68.60% test_error 70.60%\n",
      "39 acc_train_loss 623.36 acc_train_error 67.70% test_error 69.80%\n",
      "40 acc_train_loss 614.29 acc_train_error 67.20% test_error 69.20%\n",
      "41 acc_train_loss 607.62 acc_train_error 66.60% test_error 68.70%\n",
      "42 acc_train_loss 602.23 acc_train_error 65.30% test_error 67.20%\n",
      "43 acc_train_loss 596.84 acc_train_error 62.90% test_error 64.90%\n",
      "44 acc_train_loss 590.66 acc_train_error 61.40% test_error 62.20%\n",
      "45 acc_train_loss 583.46 acc_train_error 57.20% test_error 60.00%\n",
      "46 acc_train_loss 575.50 acc_train_error 54.40% test_error 56.70%\n",
      "47 acc_train_loss 567.42 acc_train_error 50.30% test_error 54.90%\n",
      "48 acc_train_loss 560.01 acc_train_error 48.30% test_error 53.40%\n",
      "49 acc_train_loss 553.79 acc_train_error 47.20% test_error 52.30%\n",
      "50 acc_train_loss 548.79 acc_train_error 46.10% test_error 51.00%\n",
      "51 acc_train_loss 544.62 acc_train_error 45.70% test_error 50.10%\n",
      "52 acc_train_loss 540.80 acc_train_error 45.10% test_error 48.80%\n",
      "53 acc_train_loss 536.91 acc_train_error 44.50% test_error 48.20%\n",
      "54 acc_train_loss 532.70 acc_train_error 43.40% test_error 47.20%\n",
      "55 acc_train_loss 528.03 acc_train_error 42.90% test_error 46.10%\n",
      "56 acc_train_loss 522.84 acc_train_error 42.60% test_error 45.40%\n",
      "57 acc_train_loss 517.17 acc_train_error 42.00% test_error 44.50%\n",
      "58 acc_train_loss 511.12 acc_train_error 41.20% test_error 43.80%\n",
      "59 acc_train_loss 504.82 acc_train_error 40.80% test_error 43.20%\n",
      "60 acc_train_loss 498.40 acc_train_error 39.80% test_error 41.90%\n",
      "61 acc_train_loss 491.95 acc_train_error 38.50% test_error 41.00%\n",
      "62 acc_train_loss 485.50 acc_train_error 36.90% test_error 39.80%\n",
      "63 acc_train_loss 479.02 acc_train_error 35.20% test_error 38.80%\n",
      "64 acc_train_loss 472.42 acc_train_error 33.30% test_error 37.50%\n",
      "65 acc_train_loss 465.65 acc_train_error 32.10% test_error 36.20%\n",
      "66 acc_train_loss 458.66 acc_train_error 31.10% test_error 35.60%\n",
      "67 acc_train_loss 451.50 acc_train_error 30.40% test_error 33.70%\n",
      "68 acc_train_loss 444.24 acc_train_error 30.00% test_error 33.60%\n",
      "69 acc_train_loss 436.99 acc_train_error 28.90% test_error 32.80%\n",
      "70 acc_train_loss 429.87 acc_train_error 27.70% test_error 32.40%\n",
      "71 acc_train_loss 423.02 acc_train_error 26.40% test_error 31.70%\n",
      "72 acc_train_loss 416.53 acc_train_error 25.20% test_error 30.90%\n",
      "73 acc_train_loss 410.46 acc_train_error 24.30% test_error 30.40%\n",
      "74 acc_train_loss 404.86 acc_train_error 23.90% test_error 30.40%\n",
      "75 acc_train_loss 399.74 acc_train_error 23.20% test_error 30.30%\n",
      "76 acc_train_loss 395.08 acc_train_error 22.60% test_error 29.40%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_662/2575705873.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0mgradwrtoutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtrain_target\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdl_dw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdl_db\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         \u001b[0mdl_dw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdl_db\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mgradwrtoutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#(w1, b1, w2, b2,train_target[n],x0, s1, x1, s2, x2,dl_dw1, dl_db1, dl_dw2, dl_db2)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0;31m# Gradient step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_662/236177204.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, *gradwrtoutput)\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mout_dl_db\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdl_db2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0mdl_dx1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mParameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdsigma\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdl_ds2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#Parameters[N-i].param()[0].t().mv(dl_ds2)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_input, train_target, test_input, test_target = prologue.load_data(one_hot_labels = True,\n",
    "                                                                        normalize = True)\n",
    "\n",
    "nb_classes = train_target.size(1)\n",
    "nb_train_samples = train_input.size(0)\n",
    "\n",
    "zeta = 0.90\n",
    "\n",
    "train_target = train_target * zeta\n",
    "test_target = test_target * zeta\n",
    "\n",
    "nb_hidden = 50\n",
    "eta = 1e-1 / nb_train_samples\n",
    "epsilon = 1e-6\n",
    "\n",
    "\n",
    "#w1 = torch.empty(nb_hidden, train_input.size(1)).normal_(0, epsilon)\n",
    "#b1 = torch.empty(nb_hidden).normal_(0, epsilon)\n",
    "#w2 = torch.empty(nb_classes, nb_hidden).normal_(0, epsilon)\n",
    "#b2 = torch.empty(nb_classes).normal_(0, epsilon)\n",
    "\n",
    "#Weights,Biais,Activation = net.param() #à définir \n",
    "#Weights= [w1,w2]\n",
    "#Biais = [b1,b2]\n",
    "#Parameters = [Linear (train_input.size(1),(nb_hidden)),Linear( nb_hidden,nb_classes)]\n",
    "#Activation = [Tanh(),Tanh()]\n",
    "net = Sequential(Linear (train_input.size(1),(nb_hidden)),Tanh(),Linear( nb_hidden,nb_classes),Tanh()).init_net()\n",
    "#[Parameters,Activation] = net.param()\n",
    "#print(net.param())\n",
    "loss = LossMSE()\n",
    "dl_dw = [torch.empty(p.param()[0].size()) for p in net.param()]#[torch.empty(w.size()) for w in Weights]\n",
    "dl_db = [torch.empty(p.param()[1].size()) for p in net.param()]#[torch.empty(b.size()) for b in Biais]\n",
    "\n",
    "#dl_dw1 = torch.empty(w1.size())\n",
    "#dl_db1 = torch.empty(b1.size())\n",
    "#dl_dw2 = torch.empty(w2.size())\n",
    "#dl_db2 = torch.empty(b2.size())\n",
    "\n",
    "for k in range(1000):\n",
    "\n",
    "    # Back-prop\n",
    "\n",
    "    acc_loss = 0\n",
    "    nb_train_errors = 0\n",
    "    \n",
    "    for dw in dl_dw:\n",
    "        dw.zero_()\n",
    "    #dl_dw1.zero_()\n",
    "    #dl_dw2.zero_()\n",
    "    for db in dl_db:\n",
    "        db.zero_()\n",
    "    #dl_db1.zero_()\n",
    "    #dl_db2.zero_()\n",
    "\n",
    "    for n in range(nb_train_samples):\n",
    "        input =  train_input[n]\n",
    "        x,s = net.forward(input)\n",
    "        \n",
    "        x2 = x[-1]\n",
    "        pred = x2.max(0)[1].item()\n",
    "        if train_target[n, pred] < 0.5:\n",
    "            nb_train_errors = nb_train_errors + 1\n",
    "        acc_loss = acc_loss + loss.sigma(x2, train_target[n])\n",
    "        \n",
    "        \n",
    "        gradwrtoutput = [train_target[n],[x,s],dl_dw,dl_db,loss]\n",
    "        dl_dw,dl_db=net.backward(*gradwrtoutput)#(w1, b1, w2, b2,train_target[n],x0, s1, x1, s2, x2,dl_dw1, dl_db1, dl_dw2, dl_db2)\n",
    "\n",
    "    # Gradient step\n",
    "   \n",
    "    for i in range(len(net.param())):\n",
    "        new_w=net.param()[i].param()[0]-eta* dl_dw[i]#Weights[i]= Weights[i]-eta * dl_dw[i]\n",
    "        new_b=net.param()[i].param()[1]-eta* dl_db[i]#Biais[i]= Biais[i]-eta * dl_db[i]\n",
    "        net.set_param(i,new_w,new_b)\n",
    "        #w1 = w1 - eta * dl_dw1\n",
    "        #b1 = b1 - eta * dl_db1\n",
    "        #w2 = w2 - eta * dl_dw2\n",
    "        #b2 = b2 - eta * dl_db2\n",
    "    \n",
    "    \n",
    "    # Test error\n",
    "\n",
    "    nb_test_errors = 0\n",
    "    \n",
    "    for n in range(test_input.size(0)):\n",
    "        input = test_input[n]\n",
    "        x,s = net.forward(input)\n",
    "        x2 = x[-1]\n",
    "        pred = x2.max(0)[1].item()\n",
    "        if test_target[n, pred] < 0.5:\n",
    "            nb_test_errors = nb_test_errors + 1\n",
    "\n",
    "    print('{:d} acc_train_loss {:.02f} acc_train_error {:.02f}% test_error {:.02f}%'\n",
    "          .format(k,\n",
    "                  acc_loss,\n",
    "                  (100 * nb_train_errors) / train_input.size(0),\n",
    "                  (100 * nb_test_errors) / test_input.size(0)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06070fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
