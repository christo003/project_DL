{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "880a3b72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x7f685061e160>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Written by Christopher Straub\n",
    "# inspired from Francois Fleuret <francois@fleuret.org> code (practical3)\n",
    "\n",
    "import math\n",
    "import torch\n",
    "import numpy as np\n",
    "import dlc_practical_prologue as prologue\n",
    "\n",
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85659c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Module(object):\n",
    "    def forward(self, *input):\n",
    "        raise NotImplementedError\n",
    "    def backward(self, *gradwrtoutput): \n",
    "        raise NotImplementedError\n",
    "    def param(self): \n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ee468d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(Module):\n",
    "    def __init__(self,input_size, hidden_size):\n",
    "        super().__init__()\n",
    "        epsilon = 1e-6\n",
    "          \n",
    "        self.weights=(torch.empty( hidden_size,input_size).normal_(0,epsilon))#Weight\n",
    "        self.biais=( torch.empty(hidden_size).normal_(0,epsilon)) #bias\n",
    "\n",
    "    def sigma(self,x):\n",
    "        return self.weights.mv(x)+self.biais\n",
    "    def dsigma(self,x):\n",
    "        return self.weights.t().mv(x)\n",
    "    def param(self):\n",
    "        return [self.weights,self.biais]\n",
    "    def set_param(self,new_w,new_b):\n",
    "        self.weights= new_w\n",
    "        self.biais = new_b\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e45fb139",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tanh(Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    def sigma(self,x):\n",
    "        return x.tanh()\n",
    "    def dsigma(self,x):\n",
    "        return 4 * (x.exp() + x.mul(-1).exp()).pow(-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b6fef23",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossMSE(Module):  \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    def sigma(self,x,y):\n",
    "        return (x - y).pow(2).sum() \n",
    "    def dsigma(self,x,y):\n",
    "        return 2*(x - y)\n",
    "   \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e3927d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.Parameters = []\n",
    "        self.Activation = []\n",
    "        self.dl_dw = []\n",
    "        self.dl_db = []\n",
    "    def forward(self,*input):\n",
    "        train_input=input[0]\n",
    "        x = train_input\n",
    "        out_s = []\n",
    "        out_x = [train_input]\n",
    "        for i in range(len(self.Activation)):\n",
    "            s = self.Parameters[i].sigma(x)\n",
    "            out_s.append(s)\n",
    "            x = self.Activation[i].sigma(s)\n",
    "            out_x.append(x)\n",
    "        return [out_x,out_s]\n",
    "\n",
    "    def backward(self,*gradwrtoutput):\n",
    "        train_target,layer_output,Loss = gradwrtoutput\n",
    "        dl_dw = self.dl_dw\n",
    "        dl_db = self.dl_db\n",
    "        N=len(dl_dw)\n",
    "        x,s=layer_output\n",
    "        x0 = x[0]\n",
    "        dl_dx2 = Loss.dsigma(x[N], train_target)\n",
    "        dl_ds2 = self.Activation[N-1].dsigma(s[N-1]) * dl_dx2\n",
    "        dl_dw2 = dl_dw[N-1]\n",
    "        dl_db2 = dl_db[N-1]\n",
    "        dl_dw2.add_(dl_ds2.view(-1, 1).mm(x[N-1].view(1, -1)))\n",
    "        dl_db2.add_(dl_ds2)\n",
    "        out_dl_dw = [dl_dw2]\n",
    "        out_dl_db = [dl_db2]\n",
    "\n",
    "        for i in range(1,N,1):\n",
    "\n",
    "            dl_dx1 = self.Parameters[N-i].dsigma(dl_ds2)\n",
    "            dl_ds1 = self.Activation[N-1-i].dsigma(s[N-1-i]) * dl_dx1\n",
    "            dl_dw1 = dl_dw[N-1-i]\n",
    "            dl_db1 = dl_db[N-1-i]\n",
    "            dl_dw1.add_(dl_ds1.view(-1, 1).mm(x[N-1-i].view(1, -1)))\n",
    "            dl_db1.add_(dl_ds1)\n",
    "            out_dl_dw.insert(0,dl_dw1)\n",
    "            out_dl_db.insert(0,dl_db1)\n",
    "            dl_ds2 = dl_ds1\n",
    "        self.dl_dw,self.dl_db = out_dl_dw,out_dl_db\n",
    "    def param(self):\n",
    "        return self.Parameters\n",
    "    def init(self,new_Parameters,new_Activation):\n",
    "        self.Parameters= new_Parameters\n",
    "        self.Activation = new_Activation\n",
    "        self.dl_dw = [torch.empty(p.param()[0].size()) for p in new_Parameters]\n",
    "        self.dl_db = [torch.empty(p.param()[1].size()) for p in new_Parameters]\n",
    "    def set_param(self,num_layer,new_w,new_b):\n",
    "        self.Parameters[num_layer].set_param(new_w,new_b)\n",
    "    def get_grad(self,num_layer):\n",
    "        return [self.dl_dw[num_layer],self.dl_db[num_layer]]\n",
    "    def zero_grad(self):\n",
    "        for dw in self.dl_dw:\n",
    "            dw.zero_()\n",
    "        for db in self.dl_db:\n",
    "            db.zero_()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b2dc9741",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequential(Module):\n",
    "    def __init__(self,*layers):\n",
    "        super().__init__()\n",
    "        self.layers = layers \n",
    "    def init_net(self):\n",
    "        net = Net()\n",
    "        new_Parameters=[]\n",
    "        new_Activation=[]\n",
    "        i = 0\n",
    "        for layer in self.layers : \n",
    "            if np.mod(i,2)==0:\n",
    "                new_Parameters.append(layer)\n",
    "            else : \n",
    "                new_Activation.append(layer)\n",
    "            i=i+1\n",
    "        net.init(new_Parameters,new_Activation)\n",
    "        return net "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e76ae257",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Using MNIST\n",
      "** Reduce the data-set (use --full for the full thing)\n",
      "** Use 1000 train and 1000 test samples\n",
      "0 acc_train_loss 810.00 acc_train_error 90.10% test_error 90.10%\n",
      "1 acc_train_loss 780.59 acc_train_error 88.30% test_error 90.10%\n",
      "2 acc_train_loss 761.77 acc_train_error 88.30% test_error 90.10%\n",
      "3 acc_train_loss 749.74 acc_train_error 88.30% test_error 90.10%\n",
      "4 acc_train_loss 742.04 acc_train_error 88.30% test_error 90.10%\n",
      "5 acc_train_loss 737.11 acc_train_error 88.30% test_error 90.10%\n",
      "6 acc_train_loss 733.95 acc_train_error 88.30% test_error 90.10%\n",
      "7 acc_train_loss 731.92 acc_train_error 88.30% test_error 90.10%\n",
      "8 acc_train_loss 730.62 acc_train_error 88.30% test_error 90.10%\n",
      "9 acc_train_loss 729.78 acc_train_error 88.30% test_error 90.10%\n",
      "10 acc_train_loss 729.25 acc_train_error 88.30% test_error 90.10%\n",
      "11 acc_train_loss 728.90 acc_train_error 88.30% test_error 90.10%\n",
      "12 acc_train_loss 728.68 acc_train_error 88.30% test_error 90.10%\n",
      "13 acc_train_loss 728.53 acc_train_error 88.30% test_error 90.10%\n",
      "14 acc_train_loss 728.44 acc_train_error 88.30% test_error 90.10%\n",
      "15 acc_train_loss 728.38 acc_train_error 88.30% test_error 90.10%\n",
      "16 acc_train_loss 728.34 acc_train_error 88.30% test_error 90.10%\n",
      "17 acc_train_loss 728.32 acc_train_error 88.30% test_error 90.10%\n",
      "18 acc_train_loss 728.30 acc_train_error 88.30% test_error 90.10%\n",
      "19 acc_train_loss 728.28 acc_train_error 88.30% test_error 90.10%\n",
      "20 acc_train_loss 728.27 acc_train_error 88.30% test_error 90.10%\n",
      "21 acc_train_loss 728.25 acc_train_error 88.30% test_error 90.10%\n",
      "22 acc_train_loss 728.22 acc_train_error 88.30% test_error 90.10%\n",
      "23 acc_train_loss 728.18 acc_train_error 88.30% test_error 90.10%\n",
      "24 acc_train_loss 728.11 acc_train_error 88.30% test_error 90.10%\n",
      "25 acc_train_loss 727.98 acc_train_error 88.30% test_error 88.60%\n",
      "26 acc_train_loss 727.75 acc_train_error 82.80% test_error 80.10%\n",
      "27 acc_train_loss 727.35 acc_train_error 78.70% test_error 79.10%\n",
      "28 acc_train_loss 726.64 acc_train_error 78.30% test_error 79.50%\n",
      "29 acc_train_loss 725.40 acc_train_error 78.90% test_error 79.30%\n",
      "30 acc_train_loss 723.27 acc_train_error 76.60% test_error 75.20%\n",
      "31 acc_train_loss 719.69 acc_train_error 71.30% test_error 73.10%\n",
      "32 acc_train_loss 713.92 acc_train_error 70.60% test_error 73.30%\n",
      "33 acc_train_loss 705.22 acc_train_error 71.10% test_error 73.90%\n",
      "34 acc_train_loss 693.27 acc_train_error 71.60% test_error 73.90%\n",
      "35 acc_train_loss 678.69 acc_train_error 71.50% test_error 73.00%\n",
      "36 acc_train_loss 663.06 acc_train_error 70.60% test_error 72.20%\n",
      "37 acc_train_loss 647.94 acc_train_error 69.50% test_error 71.60%\n",
      "38 acc_train_loss 634.29 acc_train_error 68.60% test_error 70.70%\n",
      "39 acc_train_loss 622.88 acc_train_error 67.60% test_error 69.80%\n",
      "40 acc_train_loss 614.37 acc_train_error 67.10% test_error 69.60%\n",
      "41 acc_train_loss 608.38 acc_train_error 66.60% test_error 69.20%\n",
      "42 acc_train_loss 603.63 acc_train_error 66.10% test_error 67.90%\n",
      "43 acc_train_loss 598.86 acc_train_error 64.20% test_error 66.10%\n",
      "44 acc_train_loss 593.31 acc_train_error 61.90% test_error 63.40%\n",
      "45 acc_train_loss 586.69 acc_train_error 58.90% test_error 60.90%\n",
      "46 acc_train_loss 579.07 acc_train_error 55.80% test_error 57.70%\n",
      "47 acc_train_loss 570.95 acc_train_error 52.20% test_error 55.80%\n",
      "48 acc_train_loss 563.10 acc_train_error 49.40% test_error 54.40%\n",
      "49 acc_train_loss 556.23 acc_train_error 47.80% test_error 53.10%\n",
      "50 acc_train_loss 550.63 acc_train_error 46.80% test_error 51.80%\n",
      "51 acc_train_loss 546.12 acc_train_error 45.90% test_error 50.70%\n",
      "52 acc_train_loss 542.22 acc_train_error 45.50% test_error 49.90%\n",
      "53 acc_train_loss 538.49 acc_train_error 45.00% test_error 48.90%\n",
      "54 acc_train_loss 534.56 acc_train_error 44.10% test_error 48.10%\n",
      "55 acc_train_loss 530.26 acc_train_error 43.20% test_error 47.30%\n",
      "56 acc_train_loss 525.46 acc_train_error 43.10% test_error 46.20%\n",
      "57 acc_train_loss 520.14 acc_train_error 42.30% test_error 45.40%\n",
      "58 acc_train_loss 514.35 acc_train_error 41.80% test_error 44.20%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_710/4204990097.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0mgradwrtoutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtrain_target\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m         \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mgradwrtoutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#(w1, b1, w2, b2,train_target[n],x0, s1, x1, s2, x2,dl_dw1, dl_db1, dl_dw2, dl_db2)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;31m# Gradient step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_710/229969563.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, *gradwrtoutput)\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0mdl_db1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdl_db\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0mdl_dw1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdl_ds1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m             \u001b[0mdl_db1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdl_ds1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m             \u001b[0mout_dl_dw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdl_dw1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0mout_dl_db\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdl_db1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_input, train_target, test_input, test_target = prologue.load_data(one_hot_labels = True,\n",
    "                                                                        normalize = True)\n",
    "\n",
    "nb_classes = train_target.size(1)\n",
    "nb_train_samples = train_input.size(0)\n",
    "\n",
    "zeta = 0.90\n",
    "\n",
    "train_target = train_target * zeta\n",
    "test_target = test_target * zeta\n",
    "\n",
    "nb_hidden = 50\n",
    "eta = 1e-1 / nb_train_samples\n",
    "epsilon = 1e-6\n",
    "\n",
    "\n",
    "net = Sequential(Linear (train_input.size(1),(nb_hidden)),Tanh(),Linear( nb_hidden,nb_classes),Tanh()).init_net()\n",
    "\n",
    "loss = LossMSE()\n",
    "\n",
    "for k in range(1000):\n",
    "\n",
    "    # Back-prop\n",
    "\n",
    "    acc_loss = 0\n",
    "    nb_train_errors = 0\n",
    "    \n",
    "    net.zero_grad()\n",
    "\n",
    "\n",
    "    for n in range(nb_train_samples):\n",
    "        input =  train_input[n]\n",
    "        x,s = net.forward(input)\n",
    "        \n",
    "        x2 = x[-1]\n",
    "        pred = x2.max(0)[1].item()\n",
    "        if train_target[n, pred] < 0.5:\n",
    "            nb_train_errors = nb_train_errors + 1\n",
    "        acc_loss = acc_loss + loss.sigma(x2, train_target[n])\n",
    "        \n",
    "        \n",
    "        gradwrtoutput = [train_target[n],[x,s],loss]\n",
    "        net.backward(*gradwrtoutput)\n",
    "\n",
    "    # Gradient step\n",
    "   \n",
    "    for i in range(len(net.param())):\n",
    "        dl_dw ,dl_db = net.get_grad(i)\n",
    "        new_w=net.param()[i].param()[0]-eta * dl_dw \n",
    "        new_b=net.param()[i].param()[1]-eta* dl_db \n",
    "        net.set_param(i,new_w,new_b)\n",
    "\n",
    "    \n",
    "    # Test error\n",
    "\n",
    "    nb_test_errors = 0\n",
    "    \n",
    "    for n in range(test_input.size(0)):\n",
    "        input = test_input[n]\n",
    "        x,s = net.forward(input)\n",
    "        x2 = x[-1]\n",
    "        pred = x2.max(0)[1].item()\n",
    "        if test_target[n, pred] < 0.5:\n",
    "            nb_test_errors = nb_test_errors + 1\n",
    "\n",
    "    print('{:d} acc_train_loss {:.02f} acc_train_error {:.02f}% test_error {:.02f}%'\n",
    "          .format(k,\n",
    "                  acc_loss,\n",
    "                  (100 * nb_train_errors) / train_input.size(0),\n",
    "                  (100 * nb_test_errors) / test_input.size(0)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "06070fc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1000, 784])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "train_input.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40692037",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
