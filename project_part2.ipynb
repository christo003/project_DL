{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "880a3b72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x7fa94466e460>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Written by Christopher Straub\n",
    "# inspired from Francois Fleuret <francois@fleuret.org> code (practical3)\n",
    "\n",
    "import math\n",
    "from torch import empty\n",
    "from torch import set_grad_enabled\n",
    "import numpy as np\n",
    "import dlc_practical_prologue as prologue\n",
    "\n",
    "set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85659c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Module(object):\n",
    "    def forward(self, *input):\n",
    "        raise NotImplementedError\n",
    "    def backward(self, *gradwrtoutput): \n",
    "        raise NotImplementedError\n",
    "    def param(self): \n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ee468d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(Module):\n",
    "    def __init__(self,input_size, hidden_size):\n",
    "        super().__init__()\n",
    "        epsilon = 1e-6\n",
    "        self.hidden_size = hidden_size\n",
    "        self.input_size = input_size\n",
    "        self.weights=(empty( hidden_size,input_size).normal_(0,epsilon))#Weight\n",
    "        self.biais=( empty(hidden_size).normal_(0,epsilon)) #bias\n",
    "\n",
    "    def sigma(self,x):\n",
    "        out = 0\n",
    "        if len(x.size())>1:\n",
    "            #print(self.weights.mm(x.T).size())\n",
    "            #print(self.biais.size())\n",
    "            #print(self.biais.view(-1,1)*(empty(self.hidden_size,x.size(0)).zero_().add(1)))\n",
    "            \n",
    "            out = self.weights.mm(x)+self.biais.view(-1,1)*(empty(self.hidden_size,x.size(1)).zero_().add(1))\n",
    "        else : \n",
    "            out = self.weights.mv(x)+self.biais\n",
    "        return out \n",
    "    def dsigma(self,x):\n",
    "        out = 0\n",
    "        if len(x.size())>1:\n",
    "            out = self.weights.t().mm(x.T)\n",
    "        else : \n",
    "            out = self.weights.t().mv(x)\n",
    "            \n",
    "        return out\n",
    "    def param(self):\n",
    "        return [self.weights,self.biais]\n",
    "    def set_param(self,new_w,new_b):\n",
    "        self.weights= new_w\n",
    "        self.biais = new_b\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e45fb139",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tanh(Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    def sigma(self,x):\n",
    "        return x.tanh()\n",
    "    def dsigma(self,x):\n",
    "        return 4 * (x.exp() + x.mul(-1).exp()).pow(-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b6fef23",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSE(Module):  \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = []\n",
    "    def sigma(self,x,y):\n",
    "        return (x - y).pow(2).sum()\n",
    "    def dsigma(self,x,y):\n",
    "        return 2*(x - y)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0bd67cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss(Module):  \n",
    "    def __init__(self,loss,net):\n",
    "        super().__init__()\n",
    "        self.net = net\n",
    "        self.loss = loss\n",
    "        self.acc_loss=0\n",
    "        self.nb_train_errors=0\n",
    "    def sigma(self,x,y):\n",
    "        return self.loss.sigma(x,y)\n",
    "    def dsigma(self,x,y):\n",
    "        return self.dloss.sigma(x,y)\n",
    "    def assign(self, net):\n",
    "        self.net =net \n",
    "    #def prediction(self, *input):\n",
    "    #    return self.loss.sigma(self.net.forward_value)\n",
    "    \n",
    "    def backward(self,*gradwrtoutput): \n",
    "        x,s = self.net.forward(self.net.train)\n",
    "        x2 = x[-1]\n",
    "        pred = x2.max(0)[1]\n",
    "        pred[pred<0.5]=1\n",
    "        self.nb_train_errors = pred.sum()\n",
    "        \n",
    "        self.acc_loss += self.loss.sigma(x2, self.net.train_target)\n",
    "        \n",
    "        gradwrtoutput = [self.net.train_target,[x,s],self.loss]\n",
    "        self.net.backward(*gradwrtoutput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "51e38a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossEntropy(Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    def sigma(self,x,y):\n",
    "        y_=y.argmax().item()\n",
    "        return -(x[y_].exp().div(x.exp().sum()).log())\n",
    "    def dsigma(self,x,y):\n",
    "        y_=y.argmax().item()\n",
    "        out= x.exp().div(x.exp().sum())\n",
    "        out[y_]=1-x[y_].exp().div(x.exp().sum())\n",
    "        return out \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "461dfcd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Relu( Module ) :\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    def sigma(self,x):\n",
    "        return x.max(empty(x.size()).zero_())\n",
    "    def dsigma(self,x):\n",
    "        out = x\n",
    "        out[x>0]=1\n",
    "        return out\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e3927d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.Parameters = []\n",
    "        self.Activation = []\n",
    "        self.dl_dw = []\n",
    "        self.dl_db = []\n",
    "        self.train = []\n",
    "        self.train_target = []\n",
    "        self.forward_value=[]\n",
    "        self.num_sample = 0\n",
    "    def forward(self,*input):\n",
    "        train_input=input[0]\n",
    "        x = train_input\n",
    "        out_s = []\n",
    "        out_x = [train_input]\n",
    "        for i in range(len(self.Activation)):\n",
    "            s = self.Parameters[i].sigma(x)\n",
    "            out_s.append(s)\n",
    "            x = self.Activation[i].sigma(s)\n",
    "            out_x.append(x)\n",
    "        return [out_x,out_s]\n",
    "\n",
    "    def backward(self,*gradwrtoutput):\n",
    "        train_target,layer_output,Loss = gradwrtoutput\n",
    "        dl_dw = self.dl_dw\n",
    "        dl_db = self.dl_db\n",
    "        N=len(dl_dw)\n",
    "        x,s=layer_output\n",
    "        x0 = x[0]\n",
    "        #print(x[N].size())\n",
    "        dl_dx2 = Loss.dsigma(x[N].view(-1), train_target)\n",
    "        dl_ds2 = self.Activation[N-1].dsigma(s[N-1]) * dl_dx2\n",
    "        dl_dw2 = dl_dw[N-1]\n",
    "        dl_db2 = dl_db[N-1]\n",
    "        print(dl_ds2.size())\n",
    "        print(x[N-1].size())\n",
    "        #print(dl_dw2.size())\n",
    "        dl_dw2.add_(dl_ds2.view(-1, 1).mm(x[N-1].view(1, -1)))\n",
    "        dl_db2.add_(dl_ds2)\n",
    "        out_dl_dw = [dl_dw2]\n",
    "        out_dl_db = [dl_db2]\n",
    "\n",
    "        for i in range(1,N,1):\n",
    "\n",
    "            dl_dx1 = self.Parameters[N-i].dsigma(dl_ds2) #c'est pas trÃ¨s jolie mais par Parameters je veux dire la fonction devrivant les poids de la couche cache (ici linear)\n",
    "            dl_ds1 = self.Activation[N-1-i].dsigma(s[N-1-i]) * dl_dx1\n",
    "            dl_dw1 = dl_dw[N-1-i]\n",
    "            dl_db1 = dl_db[N-1-i]\n",
    "            \n",
    "            dl_dw1.add_(dl_ds1.view(-1, 1).mm(x[N-1-i].view(1, -1)))\n",
    "            dl_db1.add_(dl_ds1)\n",
    "            out_dl_dw.insert(0,dl_dw1)\n",
    "            out_dl_db.insert(0,dl_db1)\n",
    "            dl_ds2 = dl_ds1\n",
    "        self.dl_dw,self.dl_db = out_dl_dw,out_dl_db\n",
    "    def param(self):\n",
    "        return self.Parameters\n",
    "    def init(self,new_Parameters,new_Activation):\n",
    "        self.Parameters= new_Parameters\n",
    "        self.Activation = new_Activation\n",
    "\n",
    "    def set_param(self,num_layer,new_w,new_b):\n",
    "        self.Parameters[num_layer].set_param(new_w,new_b)\n",
    "    def get_grad(self,num_layer):\n",
    "        return [self.dl_dw[num_layer],self.dl_db[num_layer]]\n",
    "    def zero_grad(self):\n",
    "        for dw in self.dl_dw:\n",
    "            dw.zero_()\n",
    "        for db in self.dl_db:\n",
    "            db.zero_()\n",
    "\n",
    "    def assign(self, train,train_target):\n",
    "        \n",
    "        if len(train_input.size())>1 : \n",
    "            self.train = train.t()\n",
    "            self.train_target = train_target.t()\n",
    "            self.num_sample = train.size(0)\n",
    "            self.dl_dw = [empty(p.param()[0].size(0),p.param()[0].size(1),self.num_sample) for p in self.Parameters]\n",
    "            self.dl_db = [empty(p.param()[1].size(0),self.num_sample) for p in self.Parameters]\n",
    "        else : \n",
    "            self.num_sample = 1\n",
    "            self.train = train\n",
    "            self.train_target = train_target\n",
    "            self.dl_dw = [empty(p.param()[0].size()) for p in self.Parameters]\n",
    "            self.dl_db = [empty(p.param()[1].size()) for p in self.Parameters]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2bc7ef14",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequential(Module):\n",
    "    def __init__(self,*layers):\n",
    "        super().__init__()\n",
    "        self.layers = layers \n",
    "    def init_net(self):\n",
    "        net = Net()\n",
    "        new_Parameters=[]\n",
    "        new_Activation=[]\n",
    "        i = 0\n",
    "        for layer in self.layers : \n",
    "            if np.mod(i,2)==0:\n",
    "                new_Parameters.append(layer)\n",
    "            else : \n",
    "                new_Activation.append(layer)\n",
    "            i=i+1\n",
    "        net.init(new_Parameters,new_Activation)\n",
    "        return net "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3b46d690",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(a):\n",
    "    num_class = a.max()+1\n",
    "    N=a.size(0)\n",
    "    out = empty(N,num_class).zero_()\n",
    "    for i in range(N) :\n",
    "        out[i][a[i]]=1 \n",
    "    return out \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "926ae9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_disc_set(nb):\n",
    "    input = empty(nb, 2).uniform_(-1, 1)\n",
    "    target = input.pow(2).sum(1).sub(2 / math.pi).sign().add(1).div(2).long()\n",
    "    \n",
    "    return input, target#one_hot(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e76ae257",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 100])\n",
      "torch.Size([25, 100])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (100) must match the size of tensor b (2500) at non-singleton dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1833/3020145797.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnarrow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmini_batch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_target\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnarrow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmini_batch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;31m# Gradient step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_1833/1269865182.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, *gradwrtoutput)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mgradwrtoutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_target\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mgradwrtoutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_1833/2233289309.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, *gradwrtoutput)\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;31m#print(dl_dw2.size())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0mdl_dw2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdl_ds2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m         \u001b[0mdl_db2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdl_ds2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mout_dl_dw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdl_dw2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (100) must match the size of tensor b (2500) at non-singleton dimension 2"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "train_input, train_target = generate_disc_set(1000)\n",
    "test_input, test_target = generate_disc_set(1000)\n",
    "\n",
    "mean, std = train_input.mean(), train_input.std()\n",
    "\n",
    "train_input.sub_(mean).div_(std)\n",
    "test_input.sub_(mean).div_(std)\n",
    "\n",
    "mini_batch_size = 100\n",
    "\n",
    "\n",
    "nb_classes = 1#train_target.size(1)\n",
    "nb_train_samples = train_input.size(0)\n",
    "\n",
    "zeta = 0.90\n",
    "\n",
    "\n",
    "\n",
    "nb_hidden = 25\n",
    "eta = 1e-1 / nb_train_samples\n",
    "epsilon = 1e-6\n",
    "\n",
    "\n",
    "net = Sequential(Linear (train_input.size(1),25),Tanh(),Linear( 25,25),Relu(),Linear( 25,nb_classes),Tanh()).init_net()\n",
    "\n",
    "loss = Loss(MSE(),net)\n",
    "\n",
    "\n",
    "nb_epochs = 250\n",
    "\n",
    "for e in range(nb_epochs):\n",
    "    for b in range(0, train_input.size(0), mini_batch_size):\n",
    "\n",
    "    # Back-prop\n",
    "\n",
    "\n",
    "    \n",
    "        \n",
    "\n",
    "        \n",
    "        net.assign(train_input.narrow(0, b, mini_batch_size),train_target.narrow(0, b, mini_batch_size))\n",
    "        net.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "    # Gradient step\n",
    "   \n",
    "    for i in range(len(net.param())):\n",
    "        dl_dw ,dl_db = net.get_grad(i)\n",
    "        new_w=net.param()[i].param()[0]-eta * dl_dw \n",
    "        new_b=net.param()[i].param()[1]-eta* dl_db \n",
    "        net.set_param(i,new_w,new_b)\n",
    "\n",
    "    \n",
    "    # Test error\n",
    "\n",
    "    nb_test_errors = 0\n",
    "    \n",
    "    for n in range(test_input.size(0)):\n",
    "        input = test_input[n]\n",
    "        x,s = net.forward(input)\n",
    "        x2 = x[-1]\n",
    "        #pred = x2.max(0)[1].item()\n",
    "        if (test_target[n]-x2).abs() < 0.5:\n",
    "            nb_test_errors = nb_test_errors + 1\n",
    "\n",
    "    print('{:d} acc_train_loss {:.02f} acc_train_error {:.02f}% test_error {:.02f}%'\n",
    "          .format(k,\n",
    "                  loss.acc_loss,\n",
    "                  (100 * loss.nb_train_errors) / train_input.size(0),\n",
    "                  (100 * nb_test_errors) / test_input.size(0)))\n",
    "    loss.nb_train_errors  = 0\n",
    "    loss.acc_loss=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c822c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "net.num_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b98425",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MSE logging the loss signifie -> faire un entrainement avec log(mse) ou bien apres l'entrianemetn pour l'Ã©valuation apprliquer le log ?? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc15401",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a faire le SGD ! :\n",
    "    #1 faire le asigne assigne un train qui est un batch\n",
    "# pourquoi mon erreur augmente ??? \n",
    "# que signifie 3 couche cachÃ© avec 25 unitÃ© ? \n",
    "# comment faire en sorte que le predict soit bien ? \n",
    "# vaut mieux avoir une sortie a savoir une valeur et voir si elle se rapproche de la valeur recherchÃ© ? \n",
    "# faire un plot avec avec un entrainement d'un ensemble de valeur dans un carrÃ© uniforme et voir quel sont les valeur correctment classifiÃ© \n",
    "# est ce que j'utilise vrm la puissance des tenseur ??? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe27cff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "net.train.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0bface",
   "metadata": {},
   "outputs": [],
   "source": [
    "empty(empty(3,2).size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445de6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "empty(3,6).zero_().add(1).mm(empty(6,2).zero_().add(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d4b9f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
